{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3QWLAxDSZKB"
      },
      "source": [
        "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<!--- @wandbcode{pytorch-lightning-transfer-learning-colab} -->\n",
        "\n",
        "# Transfer Learning Using PyTorch Lightning ‚ö°Ô∏è\n",
        "\n",
        "In this colab, we will extend the pipeline [here](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch-lightning/Image_Classification_using_PyTorch_Lightning.ipynb) to perform transfer learning with PyTorch Lightning. \n",
        "\n",
        "Transfer Learning is a technique where the knowledge learned while training a model for \"task\" A and can be used for \"task\" B. Here A and B can be the same deep learning tasks but on a different dataset. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btliUYFBTRad"
      },
      "source": [
        "## Setting up PyTorch Lightning and W&B \n",
        "\n",
        "For this tutorial, we need PyTorch Lightning and Weights and Biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRQ6TsKuSW1W"
      },
      "outputs": [],
      "source": [
        "!pip install wandb pytorch-lightning -qqq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlwjcoFGTkMP"
      },
      "source": [
        "You're gonna need these imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9pJ49LSTh-V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "# your favorite machine learning tracking tool\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import StanfordCars\n",
        "from torchvision.datasets.utils import download_url\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU_xxpiIUEX3"
      },
      "source": [
        "Now you'll need to login to you wandb account.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj2a1F0yTsqx"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlOc1qJqULTF"
      },
      "source": [
        "## The Dataset üíø\n",
        "\n",
        "We will be using the StanfordCars dataset to train our image classifier. It contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaF_kykKUFpk"
      },
      "outputs": [],
      "source": [
        "class StanfordCarsDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size, data_dir: str = './'):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Augmentation policy for training set\n",
        "        self.augmentation = transforms.Compose([\n",
        "              transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "              transforms.RandomRotation(degrees=15),\n",
        "              transforms.RandomHorizontalFlip(),\n",
        "              transforms.CenterCrop(size=224),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        # Preprocessing steps applied to validation and test set.\n",
        "        self.transform = transforms.Compose([\n",
        "              transforms.Resize(size=256),\n",
        "              transforms.CenterCrop(size=224),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        \n",
        "        self.num_classes = 196\n",
        "\n",
        "    def prepare_data(self):\n",
        "        pass\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # build dataset\n",
        "        dataset = StanfordCars(root=self.data_dir, download=True, split=\"train\")\n",
        "        # split dataset\n",
        "        self.train, self.val = random_split(dataset, [6500, 1644])\n",
        "\n",
        "        self.test = StanfordCars(root=self.data_dir, download=True, split=\"test\")\n",
        "        \n",
        "        self.test = random_split(self.test, [len(self.test)])[0]\n",
        "\n",
        "        self.train.dataset.transform = self.augmentation\n",
        "        self.val.dataset.transform = self.transform\n",
        "        self.test.dataset.transform = self.transform\n",
        "        \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val, batch_size=self.batch_size, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test, batch_size=self.batch_size, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwY6LvI2VfrT"
      },
      "source": [
        "## LightingModule - Define the System\n",
        "\n",
        "Let us look at the model definition to see how transfer learning can be used with PyTorch Lightning.\n",
        "In the `LitModel` class, we can use the pre-trained model provided by Torchvision as a feature extractor for our classification model. Here we are using ResNet-18. A list of pre-trained models provided by PyTorch Lightning can be found here.\n",
        "- When `pretrained=True`, we use the pre-trained weights; otherwise, the weights are initialized randomly.\n",
        "- If `.eval()` is used, then the layers are frozen. \n",
        "- A single `Linear` layer is used as the output layer. We can have multiple layers stacked over the `feature_extractor`.\n",
        "\n",
        "Setting the `transfer` argument to `True` will enable transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu0xf25aUckF"
      },
      "outputs": [],
      "source": [
        "class LitModel(pl.LightningModule):\n",
        "    def __init__(self, input_shape, num_classes, learning_rate=2e-4, transfer=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        # log hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.dim = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # transfer learning if pretrained=True\n",
        "        self.feature_extractor = models.resnet18(pretrained=transfer)\n",
        "\n",
        "        if transfer:\n",
        "            # layers are frozen by using eval()\n",
        "            self.feature_extractor.eval()\n",
        "            # freeze params\n",
        "            for param in self.feature_extractor.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "        n_sizes = self._get_conv_output(input_shape)\n",
        "\n",
        "        self.classifier = nn.Linear(n_sizes, num_classes)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = Accuracy()\n",
        "  \n",
        "    # returns the size of the output tensor going into the Linear layer from the conv block.\n",
        "    def _get_conv_output(self, shape):\n",
        "        batch_size = 1\n",
        "        tmp_input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
        "\n",
        "        output_feat = self._forward_features(tmp_input) \n",
        "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
        "        return n_size\n",
        "        \n",
        "    # returns the feature tensor from the conv block\n",
        "    def _forward_features(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        return x\n",
        "    \n",
        "    # will be used during inference\n",
        "    def forward(self, x):\n",
        "       x = self._forward_features(x)\n",
        "       x = x.view(x.size(0), -1)\n",
        "       x = self.classifier(x)\n",
        "       \n",
        "       return x\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        batch, gt = batch[0], batch[1]\n",
        "        out = self.forward(batch)\n",
        "        loss = self.criterion(out, gt)\n",
        "\n",
        "        acc = self.accuracy(out, gt)\n",
        "\n",
        "        self.log(\"train/loss\", loss)\n",
        "        self.log(\"train/acc\", acc)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        batch, gt = batch[0], batch[1]\n",
        "        out = self.forward(batch)\n",
        "        loss = self.criterion(out, gt)\n",
        "\n",
        "        self.log(\"val/loss\", loss)\n",
        "\n",
        "        acc = self.accuracy(out, gt)\n",
        "        self.log(\"val/acc\", acc)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        batch, gt = batch[0], batch[1]\n",
        "        out = self.forward(batch)\n",
        "        loss = self.criterion(out, gt)\n",
        "        \n",
        "        return {\"loss\": loss, \"outputs\": out, \"gt\": gt}\n",
        "    \n",
        "    def test_epoch_end(self, outputs):\n",
        "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "        output = torch.cat([x['outputs'] for x in outputs], dim=0)\n",
        "        \n",
        "        gts = torch.cat([x['gt'] for x in outputs], dim=0)\n",
        "        \n",
        "        self.log(\"test/loss\", loss)\n",
        "        acc = self.accuracy(output, gts)\n",
        "        self.log(\"test/acc\", acc)\n",
        "        \n",
        "        self.test_gts = gts\n",
        "        self.test_output = output\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCGQbJq7Wx6o"
      },
      "source": [
        "## Train your Model üèãÔ∏è‚Äç‚ôÇÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erqTHdpDVf6S"
      },
      "source": [
        "To train the model, we instantiate the `StanfordCarsDataModule` and the `LitModel` along with the PyTorch Lightning Trainer. To the `Trainer`, we will pass the `WandbLogger` as the logger to use W&B to track the metrics during model training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq4R84NbWcH6"
      },
      "outputs": [],
      "source": [
        "dm = StanfordCarsDataModule(batch_size=32)\n",
        "model = LitModel((3, 300, 300), 196, transfer=True)\n",
        "trainer = pl.Trainer(logger=WandbLogger(project=\"TransferLearning\"), max_epochs=10, accelerator=\"gpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fovvADjjYH9y"
      },
      "source": [
        "We are good to go! Let's train our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "InyEyFeqXcck"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model is trained, let's see how it performs on the test set"
      ],
      "metadata": {
        "id": "TncKz6NkYZfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(model, dm)"
      ],
      "metadata": {
        "id": "EO6mVfIeYdiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-gUHf58YeKp"
      },
      "source": [
        "Let's close our W&B run, so we call `wandb.finish()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvvnbOLmwArI"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The workspace generated to compare training the model from scratch vs using transfer learning is available [here](https://wandb.ai/manan-goel/StanfordCars). The conclusions that can be drawn from this are explained in detail in [this report](https://wandb.ai/wandb/wandb-lightning/reports/Transfer-Learning-Using-PyTorch-Lightning--VmlldzoyMzMxMzk4/edit)."
      ],
      "metadata": {
        "id": "m6KWT8j8Yywo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "I will encourage you to play with the code and train an image classifier with a dataset of your choice from scratch and using transfer learning. \n"
      ],
      "metadata": {
        "id": "UR4XUHNZY-si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn more about transfer learning check out these resources:\n",
        "- [Gotchas of transfer learning for image classification](https://docs.google.com/presentation/d/1s29WOQoQvBD5KoPUzE5TPcavjqno8ZgnZaSljHGGHVU/edit?usp=sharing) by Sayak Paul.\n",
        "- [Transfer Learning with Keras and Deep Learning by PyImageSearch.](https://www.pyimagesearch.com/2019/05/20/transfer-learning-with-keras-and-deep-learning/)\n",
        "- [Transfer Learning - Machine Learning's Next Frontier](https://ruder.io/transfer-learning/) by Sebastian Ruder.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T9xNuqshZMzD"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Transfer Learning Using PyTorch Lightning",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}